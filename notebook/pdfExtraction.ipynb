{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tabula-py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tabula\n",
    "import pdfplumber\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/tianma.pdf'\n",
    "# file_path = '../data/tianma_season_1.pdf'\n",
    "# file_path = '../data/浦发银行半年报.PDF'\n",
    "# pdf = pdfplumber.open(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extratPDF(file_path, table_method='pdfplumber', text_method='pdfplumber'):\n",
    "    '''先获取表格，然后处理句子\n",
    "       table_method 可选 'tabula', 'pdfplumber', None\n",
    "       text_method 可选 'pdfplumber', 'pdfminer', None\n",
    "    '''\n",
    "    if not os.path.isfile(file_path):\n",
    "        print(f'file {file_path} not found')\n",
    "        return None\n",
    "    \n",
    "    info = {'tables':[], 'text':[]}\n",
    "    pdf = pdfplumber.open(file_path)   \n",
    "    # 处理表格\n",
    "    if table_method is None:\n",
    "        pass\n",
    "    elif table_method == 'pdfplumber':\n",
    "        info['tables'] = extract_table_by_pdfplumber(pdf, text_tolerance=2)\n",
    "    elif table_method == 'tabula':\n",
    "        info['tables'] = extract_table_by_tabula(file_path)\n",
    "    else:\n",
    "        print(f'other table extract method {table_method} has not implement yet')\n",
    "    if info['tables']:\n",
    "        for i,table in enumerate(info['tables']):\n",
    "            info['tables'][i] = process_raw_table(table)\n",
    "            \n",
    "    # 处理句子\n",
    "    if text_method is None:\n",
    "        pass\n",
    "    elif text_method == 'pdfplumber':\n",
    "        info['text'] = extract_text_by_pdfplumber(pdf)\n",
    "    else:\n",
    "        print(f'other text extract method {text_method} has not implement yet')\n",
    "    if info['text']:\n",
    "        info['text'] = process_raw_text(info['text'])\n",
    "    \n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_raw_text(text, split=['。', '/n']):\n",
    "    '''处理文本：切分句子和表格的行，存储到es等\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/tianma\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../data/tianma.zip'"
      ]
     },
     "execution_count": 686,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert2csv(file_path, compress=False):\n",
    "    '''提取pdf中的表格并保存在文件同名目录下\n",
    "       每个 DataFrame 输出一个 csv 文件\n",
    "    '''\n",
    "    pdf_info = extratPDF(\n",
    "        file_path, \n",
    "        table_method='pdfplumber', \n",
    "        text_method=None)\n",
    "    tables = pdf_info['tables']\n",
    "    \n",
    "    csv_path = file_path.replace('.PDF','').replace('.pdf', '') \n",
    "    if compress:\n",
    "        zip_file = zipfile.ZipFile(csv_path + '.zip', 'w')\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(csv_path)\n",
    "        os.makedirs(csv_path)\n",
    "    for i, table in enumerate(tables):\n",
    "        table.to_csv(f'{csv_path}{os.sep}{i+1}.csv', \n",
    "                     index=False, header=False,\n",
    "                     encoding='gbk')\n",
    "        if compress:\n",
    "            zip_file.write(\n",
    "                f'{csv_path}{os.sep}{i+1}.csv',\n",
    "                f'{os.path.split(csv_path)[-1]}{os.sep}{i+1}.csv')\n",
    "    if compress:\n",
    "        zip_file.close()\n",
    "        return csv_path + '.zip'\n",
    "    return csv_path + os.sep\n",
    "\n",
    "convert2csv(file_path,compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/tianma'"
      ]
     },
     "execution_count": 678,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "('../data/tianma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2excel(file_path, save_path=None):\n",
    "    '''提取pdf中的表格并保存为 excel文件\n",
    "       每个DataFrame一个Sheet\n",
    "    '''\n",
    "    pdf_info = extratPDF(\n",
    "        file_path, \n",
    "        table_method='pdfplumber', \n",
    "        text_method=None)\n",
    "    tables = pdf_info['tables']\n",
    "    \n",
    "    excel_path = file_path.replace('.PDF','').replace('.pdf', '')\n",
    "    with pd.ExcelWriter(excel_path + '.xlsx') as excel:\n",
    "        for i, table in enumerate(tables):\n",
    "            table.to_excel(excel, f'Sheet{i+1}')\n",
    "    return excel_path + '.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 12s, sys: 4.31 s, total: 2min 16s\n",
      "Wall time: 2min 28s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../data/浦发银行半年报.xlsx'"
      ]
     },
     "execution_count": 680,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "convert2excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 46.5 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def extract_text_by_pdfplumber(file, **kargs):\n",
    "    # 处理文本，返回句子等\n",
    "    if isinstance(file, str):\n",
    "        pages = pdfplumber.open(file).pages\n",
    "    if isinstance(file, pdfplumber.pdf.PDF):\n",
    "        pages = file.pages\n",
    "        \n",
    "    text = [[]]\n",
    "    for page in pages:\n",
    "        x0,y0 = Decimal(0), Decimal(0)\n",
    "        for char in page.chars:\n",
    "            if (char['x0'] < x0-20) and (char['y0'] < y0-20):\n",
    "                print('case 1', char['text'])\n",
    "                x0, y0 = char['x0'], char['y0']\n",
    "                text.append([char['text']])\n",
    "            elif (char['x0'] > x0+1) and (char['y0'] > y0+1):\n",
    "                x0, y0 = char['x0'], char['y0']\n",
    "                text.append([char['text']])\n",
    "                print('case 2', char['text'])\n",
    "            else:\n",
    "                text[-1].append(char['text'])\n",
    "                # print(char['text'], end='')\n",
    "        # raw_text = [char['text'] for char in page.chars]\n",
    "        # text.extend(raw_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "CPU times: user 30 ms, sys: 40 ms, total: 70 ms\n",
      "Wall time: 23.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def extract_table_by_tabula(file_path, page='all'):\n",
    "    '''使用 tabula-java输出的json数据来提取表格'''\n",
    "    tables = []\n",
    "    try:\n",
    "        tables = tabula.read_pdf(\n",
    "            file_path,\n",
    "            encoding='utf-8', \n",
    "            pages='all',\n",
    "            multiple_tables=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print('Error in extract_table_by_tabula:', e)\n",
    "    finally:\n",
    "        return tables\n",
    "    \n",
    "tables_2 = extract_table_by_tabula(file_path)\n",
    "print(len(tables_2));tables_2[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "CPU times: user 11.6 s, sys: 180 ms, total: 11.8 s\n",
      "Wall time: 11.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def extract_table_by_pdfplumber(file, text_tolerance=2):\n",
    "    '''使用 pdfplumber 的两种分析模式来获取表格\n",
    "    '''\n",
    "    if isinstance(file, str):\n",
    "        pages = pdfplumber.open(file).pages\n",
    "    if isinstance(file, pdfplumber.pdf.PDF):\n",
    "        pages = file.pages\n",
    "    tables = []\n",
    "    for page in pages:\n",
    "        raw_tables = pdfplumber_by_line(page)\n",
    "        if not raw_tables:\n",
    "            raw_tables = pdfplumber_by_text(\n",
    "                page, text_tolerance=2)\n",
    "        if not raw_tables:\n",
    "            continue\n",
    "        for raw_table in raw_tables:\n",
    "            raw_table_ = raw_table.extract()\n",
    "            pd_table = pd.DataFrame(raw_table_)\n",
    "            # 有时会将较为整齐的文本当成表格，去掉这种 1*x 或 x*1的不正确的表格\n",
    "            if 1 in list(pd_table.shape):\n",
    "                continue\n",
    "            tables.append(pd_table\n",
    "                # 'bbox':raw_table.bbox\n",
    "            )\n",
    "    return tables\n",
    "\n",
    "tables_1 = extract_table_by_pdfplumber(file_path)\n",
    "print(len(tables_1));tables_1[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdfplumber_by_line(page):\n",
    "    # 根据pdf中的线条来拆分表格，一般比较精准\n",
    "    tables = page.find_tables(table_settings={\n",
    "        \"vertical_strategy\": \"lines\", \n",
    "        \"horizontal_strategy\": \"lines\",\n",
    "        \"intersection_tolerance\": 2,\n",
    "    })\n",
    "    return tables\n",
    "\n",
    "def pdfplumber_by_text(page, text_tolerance=1):\n",
    "    # 根据pdf页中的文本对齐来拆分表格，结果会更全，但不一定正确\n",
    "    tables = page.find_tables(table_settings={\n",
    "        \"vertical_strategy\": \"text\", \n",
    "        \"horizontal_strategy\": \"text\",\n",
    "        # \"intersection_tolerance\": 2,\n",
    "        \"text_tolerance\": text_tolerance,\n",
    "        # \"text_x_tolerance\": 1, # pixels\n",
    "        # \"text_y_tolerance\": 5,\n",
    "    })\n",
    "    return tables\n",
    "\n",
    "def pdfplumber_find_title(page, bbox):\n",
    "    # 根据表格的bbox位置，来查找它上面的标题\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_raw_table(table, drop_column_ratio=0.1, drop_row_ratio=0.1):\n",
    "    '''处理原始的表格，删除空白行等 \n",
    "    '''\n",
    "    if isinstance(table, list):\n",
    "        table = pd.DataFrame(table)\n",
    "    \n",
    "    # 将 None 和 空白字符 替换成 NaN\n",
    "    table = table.applymap(lambda x:x if x else np.NaN)\n",
    "    # 删除全部是空或者90%是空的行、列\n",
    "    # column_valid_ratio = table.count(axis=0)/table.shape[0]\n",
    "    # table = table.T[column_valid_ratio > drop_column_ratio].T\n",
    "    # row_valid_ratio = table.count(1)/table.shape[1]\n",
    "    # table = table[row_valid_ratio > drop_row_ratio]\n",
    "    \n",
    "    # 删除全部是空的行和列\n",
    "    table.dropna(axis=0, how='all', inplace=True)\n",
    "    table.dropna(axis=1, how='all', inplace=True)\n",
    "    \n",
    "    # 将大量空白符以及换行符等，替换成单个空白符\n",
    "    table.replace(regex=r'(：)', value=': ', inplace=True)\n",
    "    # 数字里的逗号会产生干扰\n",
    "    table.replace(regex=r'(,)', value='',inplace=True)\n",
    "    table.replace(regex=[r'(\\n)', r'(  )'], value=' ', inplace=True)\n",
    "    \n",
    "    return table\n",
    "\n",
    "# process_raw_table(tables[0].extract())\n",
    "# tables[0].extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2018年半年度报告  9 \n",
      "注： \n",
      "（1）公司根据财政部《企业会计准则第42号——持有待售非流动资产、处置组和终止经营》及《企业\n",
      "会计准则第16号——政府补助》的规定，将原计入在“营业外收入”和“营业外支出”中的相关资产处置\n",
      "利得或损失及与企业日常活动相关的政府补助分别计入新增的“资产处置损益”及“其他收益”项目，并\n",
      "对比较期相关报表项目及财务指标重述，上述规定对公司利润总额和净利润没有影响。 \n",
      "（2）基本每股收益、加权平均净资产收益率根据《公开发行证券的公司信息披露编报规则第9号——净\n",
      "资产收益率和每股收益的计算及披露（2010年修订）》计算。报告期基本每股收益按照发行在外的普通股\n",
      "加权平均数29,352,080,397股计算得出。 \n",
      "（3）2018年3月，公司对浦发优2优先股发放股息人民币8.25亿元。在计算本报告披露的每股收益及\n",
      "加权平均净资产收益率等指标时，公司考虑了浦发优2优先股股息发放的影响。 \n",
      "（4）非经常性损益根据《中国证券监督管理委员会公告2008年第43号—公开发行证券的公司信息披露\n",
      "解释性公告第1号——非经常性损益》的定义计算。 \n",
      "（5）平均总资产收益率=净利润/资产平均余额，资产平均余额＝（期初资产总额＋期末资产总额）/2。 \n",
      "（6）全面摊薄净资产收益率=报告期归属于母公司普通股股东的净利润/期末归属于母公司普通股股东的\n",
      "净资产。 \n",
      "（7）扣除非经常性损益后全面摊薄净资产收益率=报告期归属于母公司普通股股东的扣除非经常性损益\n",
      "的净利润/期末归属于母公司普通股股东的净资产。 \n",
      "（8）净利差为总生息资产平均收益率与总计息负债平均成本率两者的差额。净利息收益率=利息净收入/\n",
      "总生息资产平均余额。根据2018年1月1日实行的新金融工具准则的要求，报告期内以公允价值计量且其变\n",
      "动计入当期损益的金融资产产生的利息计入投资损益，若按原口径还原至利息收入，集团净利差为1.72%，\n",
      "净利息收益率为1.81%。 \n",
      "（9）成本收入比=业务及管理费/营业收入。 \n",
      "（10）报告期加权平均净资产收益率、扣除非经常性损益后的加权平均净资产收益率、全面摊薄净资产\n",
      "收益率、扣除非经常性损益后全面摊薄净资产收益率、平均总资产收益率未年化处理。 \n",
      "2.2企业会计准则与国际财务报告准则下会计数据差异 \n",
      "按照企业会计准则编制的财务报表和按照国际财务报告准则编制的财务报表中列示的\n",
      "报告期的本集团净利润、资产总额、负债总额无差异。 \n",
      " \n",
      "2.3非经常性损益项目和金额 \n",
      "单位：人民币百万元 \n",
      "项    目 本报告期 上年同期 \n",
      "非流动资产处置损益 6 579 \n",
      "政府补助 240 112 \n",
      "其他营业外净(支出)/收入 -3 80 \n",
      "非经常性损益的所得税影响数 -64 -193 \n",
      "合    计 179  578  \n",
      "其中：归属于母公司股东的非经常性损益 \n",
      "  137   547  归属于少数股东的非经常性损益 \n",
      "  42   31   \n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# 合并不同页面的句子？\n",
    "page_5 = pages[4]\n",
    "sentence = [[],]\n",
    "bottom = 0\n",
    "for i, char in enumerate(pages[8].chars):\n",
    "#     for key,val in char.items():\n",
    "#         print(f\"{key}:{str(val)}\", end=' | ')\n",
    "#     print('')\n",
    "#     print(char['text'], end='')\n",
    "    if float(bottom) + 200 > float(char['bottom']) > float(bottom) + 10:\n",
    "        bottom = float(char['bottom'])\n",
    "        sentence.append([char['text']])\n",
    "    else:\n",
    "        sentence[-1].append(char['text'])\n",
    "    # print(i, char['text'], end=' ')\n",
    "for s in sentence:\n",
    "    print(''.join(s))\n",
    "    #print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_rep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquoting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquotechar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\"'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_terminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtupleize_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoublequote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mescapechar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Write DataFrame to a comma-separated values (csv) file\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "path_or_buf : string or file handle, default None\n",
       "    File path or object, if None is provided the result is returned as\n",
       "    a string.\n",
       "sep : character, default ','\n",
       "    Field delimiter for the output file.\n",
       "na_rep : string, default ''\n",
       "    Missing data representation\n",
       "float_format : string, default None\n",
       "    Format string for floating point numbers\n",
       "columns : sequence, optional\n",
       "    Columns to write\n",
       "header : boolean or list of string, default True\n",
       "    Write out the column names. If a list of strings is given it is\n",
       "    assumed to be aliases for the column names\n",
       "index : boolean, default True\n",
       "    Write row names (index)\n",
       "index_label : string or sequence, or False, default None\n",
       "    Column label for index column(s) if desired. If None is given, and\n",
       "    `header` and `index` are True, then the index names are used. A\n",
       "    sequence should be given if the DataFrame uses MultiIndex.  If\n",
       "    False do not print fields for index names. Use index_label=False\n",
       "    for easier importing in R\n",
       "mode : str\n",
       "    Python write mode, default 'w'\n",
       "encoding : string, optional\n",
       "    A string representing the encoding to use in the output file,\n",
       "    defaults to 'ascii' on Python 2 and 'utf-8' on Python 3.\n",
       "compression : string, optional\n",
       "    A string representing the compression to use in the output file.\n",
       "    Allowed values are 'gzip', 'bz2', 'zip', 'xz'. This input is only\n",
       "    used when the first argument is a filename.\n",
       "line_terminator : string, default ``'\\n'``\n",
       "    The newline character or character sequence to use in the output\n",
       "    file\n",
       "quoting : optional constant from csv module\n",
       "    defaults to csv.QUOTE_MINIMAL. If you have set a `float_format`\n",
       "    then floats are converted to strings and thus csv.QUOTE_NONNUMERIC\n",
       "    will treat them as non-numeric\n",
       "quotechar : string (length 1), default '\\\"'\n",
       "    character used to quote fields\n",
       "doublequote : boolean, default True\n",
       "    Control quoting of `quotechar` inside a field\n",
       "escapechar : string (length 1), default None\n",
       "    character used to escape `sep` and `quotechar` when appropriate\n",
       "chunksize : int or None\n",
       "    rows to write at a time\n",
       "tupleize_cols : boolean, default False\n",
       "    .. deprecated:: 0.21.0\n",
       "       This argument will be removed and will always write each row\n",
       "       of the multi-index as a separate row in the CSV file.\n",
       "\n",
       "    Write MultiIndex columns as a list of tuples (if True) or in\n",
       "    the new, expanded format, where each MultiIndex column is a row\n",
       "    in the CSV (if False).\n",
       "date_format : string, default None\n",
       "    Format string for datetime objects\n",
       "decimal: string, default '.'\n",
       "    Character recognized as decimal separator. E.g. use ',' for\n",
       "    European data\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame.to_csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 "
     ]
    }
   ],
   "source": [
    "excel_path = file_path.replace('.PDF','').replace('.pdf', '')\n",
    "with pd.ExcelWriter(excel_path + '.xlsx') as excel:\n",
    "    for i, table in enumerate(ttt):\n",
    "        print(i+1,end=' ')\n",
    "        #table.to_excel(excel, f'Sheet{i+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
